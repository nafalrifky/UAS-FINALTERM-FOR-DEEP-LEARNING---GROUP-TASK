{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba8b593",
   "metadata": {},
   "source": [
    "## ============================================\n",
    "## TASK 3: DECODER-ONLY LLM FINE-TUNING ON XSUM\n",
    "## ============================================\n",
    "## Kriteria: \n",
    "## 1. Decoder-only LLM (GPT-2 sebagai PIN-2)\n",
    "## 2. XSum dataset untuk abstractive summarization  \n",
    "## 3. Instruction-style prompting\n",
    "## 4. Causal language modeling training\n",
    "## 5. Generated control pada inference\n",
    "## ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d5f5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Documents\\UAS DL\\bert_env_new\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ASUS\\Documents\\UAS DL\\bert_env_new\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\ASUS\\Documents\\UAS DL\\bert_env_new\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\ASUS\\Documents\\UAS DL\\bert_env_new\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TASK 3: FINE-TUNING DECODER-ONLY LLM FOR ABSTRACTIVE SUMMARIZATION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TASK 3: FINE-TUNING DECODER-ONLY LLM FOR ABSTRACTIVE SUMMARIZATION\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551be454",
   "metadata": {},
   "source": [
    "## 1. LOAD XSUM DATASET\n",
    "### Penjelasan:\n",
    "### Dataset XSum dari Edinburgh NLP digunakan sesuai instruksi tugas\n",
    "### Setiap sampel berisi dokumen artikel dan ringkasan satu kalimat\n",
    "### Ringkasan bersifat abstractive (bukan extractive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73449026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• 1. Loading XSum dataset from HuggingFace...\n",
      "‚úÖ Dataset loaded successfully!\n",
      "   Train samples: 204,045\n",
      "   Validation samples: 11,332\n",
      "   Test samples: 11,334\n",
      "\n",
      "üìÑ Sample data structure:\n",
      "   Document length: 2323 chars\n",
      "   Summary: Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.\n",
      "   Summary length: 126 chars\n",
      "   ID: 35232142\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüì• 1. Loading XSum dataset from HuggingFace...\")\n",
    "\n",
    "# Load dataset sesuai tugas: XSum untuk abstractive summarization\n",
    "dataset = load_dataset(\"EdinburghNLP/xsum\")\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"   Train samples: {len(dataset['train']):,}\")\n",
    "print(f\"   Validation samples: {len(dataset['validation']):,}\")\n",
    "print(f\"   Test samples: {len(dataset['test']):,}\")\n",
    "\n",
    "# Contoh data untuk memahami format\n",
    "print(\"\\nüìÑ Sample data structure:\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"   Document length: {len(sample['document'])} chars\")\n",
    "print(f\"   Summary: {sample['summary']}\")\n",
    "print(f\"   Summary length: {len(sample['summary'])} chars\")\n",
    "print(f\"   ID: {sample['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d18edf",
   "metadata": {},
   "source": [
    "## 2. PREPROCESSING WITH INSTRUCTION PROMPTING\n",
    "### Penjelasan:\n",
    "### Instruction-style prompting: Format prompt yang eksplisit dengan instruksi \"Summarize the following BBC news article...\"\n",
    "### Causal LM: Labels diset sama dengan inputs untuk next-token prediction\n",
    "### GPT-2 tokenizer: Tokenizer khusus untuk decoder-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856c05e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî§ 2. Loading tokenizer and preprocessing with instruction-style prompting...\n",
      "   Using model: distilgpt2 (smaller, faster, better for 4GB GPU)\n",
      "   Set pad_token to eos_token: <|endoftext|>\n",
      "   Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 204045/204045 [01:45<00:00, 1934.67 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11332/11332 [00:08<00:00, 1366.48 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11334/11334 [00:07<00:00, 1555.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing completed!\n",
      "   Input shape example: 512 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüî§ 2. Loading tokenizer and preprocessing with instruction-style prompting...\")\n",
    "\n",
    "MODEL_NAME = \"distilgpt2\"  # ‚¨ÖÔ∏è GANTI DARI \"gpt2\" KE \"distilgpt2\"\n",
    "print(f\"   Using model: {MODEL_NAME} (smaller, faster, better for 4GB GPU)\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set padding token (GPT-2 tidak punya pad token default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"   Set pad_token to eos_token: {tokenizer.eos_token}\")\n",
    "\n",
    "def create_instruction_prompt(document, summary=None, is_training=True):\n",
    "    \"\"\"\n",
    "    Membuat instruction prompt sesuai kriteria tugas:\n",
    "    - Instruction-style prompting\n",
    "    - Format yang jelas untuk summarization\n",
    "    \"\"\"\n",
    "    # Truncate document jika terlalu panjang\n",
    "    max_doc_length = 768  # Sesuaikan dengan GPU memory\n",
    "    truncated_doc = document[:max_doc_length] + \"...\" if len(document) > max_doc_length else document\n",
    "    \n",
    "    if is_training:\n",
    "        # Format untuk training: instruction + document + summary\n",
    "        prompt = f\"\"\"Summarize the following BBC news article into one concise sentence:\n",
    "\n",
    "{truncated_doc}\n",
    "\n",
    "Summary: {summary}\"\"\"\n",
    "    else:\n",
    "        # Format untuk inference: instruction + document\n",
    "        prompt = f\"\"\"Summarize the following BBC news article into one concise sentence:\n",
    "\n",
    "{truncated_doc}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocessing function yang BENAR untuk summarization:\n",
    "    - Input: Instruction + document\n",
    "    - Labels: Summary saja (bukan seluruh input)\n",
    "    \"\"\"\n",
    "    # Prepare inputs (hanya instruction + document, TANPA summary)\n",
    "    inputs = [create_instruction_prompt(doc, is_training=False) \n",
    "              for doc in examples['document']]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,  # Batasi panjang input\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Tokenize labels (summary saja) - PAKAI as_target_tokenizer\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['summary'],  # Hanya summary sebagai target\n",
    "            max_length=128,       # Summary lebih pendek\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    \n",
    "    # Labels adalah summary token\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"   Tokenizing datasets...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset['train'].column_names  # Hapus kolom original\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocessing completed!\")\n",
    "print(f\"   Input shape example: {len(tokenized_datasets['train'][0]['input_ids'])} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dea110",
   "metadata": {},
   "source": [
    "## 3. SPLIT DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6575ce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä 3. Splitting dataset for training...\n",
      "‚úÖ Dataset split completed!\n",
      "   Training samples: 5000\n",
      "   Validation samples: 1000\n",
      "   Test samples: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä 3. Splitting dataset for training...\")\n",
    "\n",
    "TRAIN_SUBSET_SIZE = 5000\n",
    "VAL_SUBSET_SIZE = 1000\n",
    "\n",
    "# Random sampling untuk subset\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.choice(len(tokenized_datasets['train']), TRAIN_SUBSET_SIZE, replace=False)\n",
    "val_indices = np.random.choice(len(tokenized_datasets['validation']), VAL_SUBSET_SIZE, replace=False)\n",
    "\n",
    "# Create subsets\n",
    "train_dataset = tokenized_datasets['train'].select(train_indices.tolist())\n",
    "val_dataset = tokenized_datasets['validation'].select(val_indices.tolist())\n",
    "test_dataset = tokenized_datasets['test'].select(range(500))  # 500 sampel untuk test\n",
    "\n",
    "print(f\"‚úÖ Dataset split completed!\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996aaeb",
   "metadata": {},
   "source": [
    "## 4. LOAD MODEL & SETUP TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5711889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ 4. Loading decoder-only LLM model...\n",
      "\n",
      "ü§ñ 4. Loading decoder-only LLM model (distilgpt2)...\n",
      "‚úÖ Model loaded: distilgpt2\n",
      "   Total parameters: 81,912,576 (40% lebih kecil dari GPT-2)\n",
      "‚úÖ Model loaded: distilgpt2\n",
      "   Total parameters: 81,912,576\n",
      "   Setting up data collator for causal LM...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ 4. Loading decoder-only LLM model...\")\n",
    "\n",
    "print(\"\\nü§ñ 4. Loading decoder-only LLM model (distilgpt2)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,} (40% lebih kecil dari GPT-2)\")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Setup data collator untuk causal language modeling\n",
    "print(\"   Setting up data collator for causal LM...\")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # False untuk causal language modeling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f19b9",
   "metadata": {},
   "source": [
    "## 5. TRAINING ARGUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43d7eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è  5. Setting up training arguments...\n",
      "   Setting up Trainer...\n",
      "‚úÖ Training setup completed!\n",
      "   GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "   GPU Memory: 4.3 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚öôÔ∏è  5. Setting up training arguments...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output configuration\n",
    "    output_dir=\"./models/distilgpt2-xsum-summarization\",\n",
    "    overwrite_output_dir=True,\n",
    "    run_name=f\"xsum-distilgpt2-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
    "    \n",
    "    # Training strategy - OPTIMIZED FOR 4GB GPU\n",
    "    num_train_epochs=3,  # ‚¨ÖÔ∏è NAIKKAN KE 3 EPOCH\n",
    "    per_device_train_batch_size=1,  # ‚¨ÖÔ∏è TURUNKAN KE 1 (karena GPU 4GB)\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # ‚¨ÖÔ∏è NAIKKAN AKUMULASI\n",
    "    \n",
    "    # Optimization - TUNED FOR BETTER CONVERGENCE\n",
    "    learning_rate=3e-5,  # ‚¨ÖÔ∏è LEARNING RATE LEBIH RENDAH\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,  # ‚¨ÖÔ∏è GUNAKAN RATIO BUKAN STEPS\n",
    "    \n",
    "    # Evaluation & saving - COMPATIBLE WITH OLDER VERSIONS\n",
    "    evaluation_strategy=\"steps\",  # ‚¨ÖÔ∏è GUNAKAN 'evaluation_strategy' BUKAN 'eval_strategy'\n",
    "    eval_steps=300,  # ‚¨ÖÔ∏è EVAL LEBIH JARANG\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=300,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Performance optimizations\n",
    "    fp16=True,  # Mixed precision WAJIB untuk 4GB GPU\n",
    "    gradient_checkpointing=True,  # Hemat memory\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,  # ‚¨ÖÔ∏è LOG LEBIH JARANG\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Push to hub\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Setup Trainer\n",
    "print(\"   Setting up Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training setup completed!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No GPU detected, training on CPU (will be slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8e914",
   "metadata": {},
   "source": [
    "## 6. TRAINING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca507df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ STARTING TRAINING\n",
      "üìä Model: distilgpt2\n",
      "üìà Epochs: 3\n",
      "üì¶ Batch size: 1\n",
      "üìö Training samples: 5000\n",
      "‚è±Ô∏è  Estimated time: 2-3 hours\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  5%|‚ñå         | 100/1875 [01:15<22:16,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4953, 'learning_rate': 1.5957446808510637e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 200/1875 [02:30<20:44,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1553, 'learning_rate': 2.978660343805572e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 300/1875 [03:44<19:37,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0959, 'learning_rate': 2.8008298755186724e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 16%|‚ñà‚ñå        | 300/1875 [04:09<19:37,  1.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9900248050689697, 'eval_runtime': 24.5812, 'eval_samples_per_second': 40.681, 'eval_steps_per_second': 40.681, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà‚ñè       | 400/1875 [05:26<18:34,  1.32it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.073, 'learning_rate': 2.6229994072317723e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 500/1875 [06:42<17:22,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1001, 'learning_rate': 2.4451689389448725e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 600/1875 [07:57<16:08,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0909, 'learning_rate': 2.2673384706579728e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 600/1875 [08:22<16:08,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.964846134185791, 'eval_runtime': 24.6818, 'eval_samples_per_second': 40.516, 'eval_steps_per_second': 40.516, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 700/1875 [09:38<14:53,  1.31it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0198, 'learning_rate': 2.089508002371073e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 800/1875 [10:53<13:17,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9918, 'learning_rate': 1.911677534084173e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 900/1875 [12:07<11:57,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.985, 'learning_rate': 1.733847065797273e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 900/1875 [12:31<11:57,  1.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9577434062957764, 'eval_runtime': 24.7813, 'eval_samples_per_second': 40.353, 'eval_steps_per_second': 40.353, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1000/1875 [13:47<10:53,  1.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0019, 'learning_rate': 1.5560165975103737e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1100/1875 [15:06<10:40,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9739, 'learning_rate': 1.3781861292234736e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1200/1875 [16:27<08:50,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9941, 'learning_rate': 1.2003556609365739e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1200/1875 [16:54<08:50,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.950232982635498, 'eval_runtime': 26.842, 'eval_samples_per_second': 37.255, 'eval_steps_per_second': 37.255, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1300/1875 [18:09<07:04,  1.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9624, 'learning_rate': 1.022525192649674e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1400/1875 [19:26<06:10,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9714, 'learning_rate': 8.446947243627742e-06, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1500/1875 [20:45<04:53,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9075, 'learning_rate': 6.6686425607587435e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1500/1875 [21:16<04:53,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.951547384262085, 'eval_runtime': 31.8874, 'eval_samples_per_second': 31.36, 'eval_steps_per_second': 31.36, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1600/1875 [22:35<03:33,  1.29it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9205, 'learning_rate': 4.890337877889745e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1700/1875 [23:54<02:16,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9433, 'learning_rate': 3.112033195020747e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1800/1875 [25:13<00:58,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9133, 'learning_rate': 1.3337285121517488e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1800/1875 [25:41<00:58,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.948681592941284, 'eval_runtime': 28.486, 'eval_samples_per_second': 35.105, 'eval_steps_per_second': 35.105, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1875/1875 [26:41<00:00,  1.29it/s]There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1875/1875 [26:41<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1601.9603, 'train_samples_per_second': 9.364, 'train_steps_per_second': 1.17, 'train_loss': 3.0292682454427085, 'epoch': 3.0}\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TRAINING COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "üíæ Saving fine-tuned model...\n",
      "‚úÖ Model saved to: ./models/gpt2-xsum-finetuned\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(f\"üìä Model: {MODEL_NAME}\")\n",
    "print(f\"üìà Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"üì¶ Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"üìö Training samples: {len(train_dataset)}\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: 2-3 hours\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Mulai training\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save model\n",
    "print(\"\\nüíæ Saving fine-tuned model...\")\n",
    "model_save_path = \"./models/gpt2-xsum-finetuned\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"‚úÖ Model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d14c3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies yang diperlukan untuk ROUGE\n",
    "!pip install nltk absl-py rouge-score -q\n",
    "\n",
    "# Atau jika di VS Code, buka terminal dan jalankan:\n",
    "# pip install nltk absl-py rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28d8c5",
   "metadata": {},
   "source": [
    "## 7. EVALUATION WITH GENERATED CONTROL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3cb5174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è  6.5. Setting generation config for better summaries...\n",
      "‚úÖ Generation config set!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚öôÔ∏è  6.5. Setting generation config for better summaries...\")\n",
    "\n",
    "# Set config untuk generate yang lebih baik\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Force model to use these settings\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.max_length = 150\n",
    "model.generation_config.min_length = 30\n",
    "model.generation_config.temperature = 0.8\n",
    "model.generation_config.do_sample = True\n",
    "model.generation_config.top_p = 0.9\n",
    "\n",
    "print(\"‚úÖ Generation config set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "088ea024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä EVALUATION WITH GENERATED CONTROL PARAMETERS\n",
      "======================================================================\n",
      "\n",
      "üìà Loading ROUGE metric...\n",
      "‚úÖ ROUGE metric loaded successfully\n",
      "\n",
      "üß™ Testing model with controlled generation...\n",
      "\n",
      "üìù Generated Summaries (with control parameters):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìå Contoh 1:\n",
      "üìÑ Dokumen (potongan): Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\n",
      "...\n",
      "üéØ Referensi (ground truth): There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.\n",
      "ü§ñ Prediksi (model): The Welsh government has made a number of changes to the housing act, including changes to its housing policy.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìå Contoh 2:\n",
      "üìÑ Dokumen (potongan): Officers searched properties in the Waterfront Park and Colonsay View areas of the city on Wednesday.\n",
      "Detectives said three firearms, ammunition and a...\n",
      "üéØ Referensi (ground truth): A man has appeared in court after firearms, ammunition and cash were seized by police in Edinburgh.\n",
      "ü§ñ Prediksi (model): Police said the man, who is in his 20s, was arrested on suspicion of possession of a firearm.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìå Contoh 3:\n",
      "üìÑ Dokumen (potongan): Jordan Hill, Brittany Covington and Tesfaye Cooper, all 18, and Tanishia Covington, 24, appeared in a Chicago court on Friday.\n",
      "The four have been char...\n",
      "üéØ Referensi (ground truth): Four people accused of kidnapping and torturing a mentally disabled man in a \"racially motivated\" attack streamed on Facebook have been denied bail.\n",
      "ü§ñ Prediksi (model): The three men were arrested on suspicion of attempted murder and attempted murder.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìå Contoh 4:\n",
      "üìÑ Dokumen (potongan): The 48-year-old former Arsenal goalkeeper played for the Royals for four years.\n",
      "He was appointed youth academy director in 2000 and has been director ...\n",
      "üéØ Referensi (ground truth): West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.\n",
      "ü§ñ Prediksi (model): \"He is one of the best young players in the world and we look forward to working with him on his future,\" the club said in a statement.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìå Contoh 5:\n",
      "üìÑ Dokumen (potongan): Restoring the function of the organ - which helps control blood sugar levels - reversed symptoms of diabetes in animal experiments.\n",
      "The study, publish...\n",
      "üéØ Referensi (ground truth): The pancreas can be triggered to regenerate itself through a type of fasting diet, say US researchers.\n",
      "ü§ñ Prediksi (model): The study was carried out in the laboratory at the University of Zurich.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä Calculating ROUGE scores...\n",
      "\n",
      "‚úÖ EVALUATION RESULTS:\n",
      "   ROUGE-1: 0.1687\n",
      "   ROUGE-2: 0.0000\n",
      "   ROUGE-L: 0.1256\n",
      "   ROUGE-Lsum: 0.1296\n",
      "\n",
      "üíæ Saving evaluation results...\n",
      "‚úÖ Results saved to: ./results/xsum_summarization_results.csv\n",
      "\n",
      "======================================================================\n",
      "üéõÔ∏è  DEMONSTRATION: GENERATED CONTROL PARAMETERS (Simple)\n",
      "======================================================================\n",
      "\n",
      "üìÑ Demo document (truncated): Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\n",
      "Workers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.\n",
      "The Welsh Government said more people than...\n",
      "\n",
      "üîß Effect of temperature control:\n",
      "--------------------------------------------------\n",
      "\n",
      "üå°Ô∏è  Temperature = 0.3:\n",
      "   \"We are working hard to find a place where we can help people who are living in poverty.\"\n",
      "\n",
      "üå°Ô∏è  Temperature = 0.7:\n",
      "   \"We need to make sure we can get the best out of these people.\"\n",
      "\n",
      "üå°Ô∏è  Temperature = 1.0:\n",
      "   The charity has spent more than ¬£1.5m on housing in Wales this year, up from ¬£2.8m in 2015\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EVALUATION COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä EVALUATION WITH GENERATED CONTROL PARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Import dan setup NLTK untuk ROUGE\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"üì¶ Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"‚úÖ NLTK punkt downloaded\")\n",
    "\n",
    "# Load ROUGE metric\n",
    "print(\"\\nüìà Loading ROUGE metric...\")\n",
    "try:\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    print(\"‚úÖ ROUGE metric loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error loading ROUGE: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Installing required packages...\")\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\", \"rouge-score\", \"absl-py\"])\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    print(\"‚úÖ Required packages installed and ROUGE loaded\")\n",
    "\n",
    "def generate_summary_with_control(text, model, tokenizer, control_params=None):\n",
    "    \"\"\"\n",
    "    Generate summary dengan kontrol parameter sesuai kriteria:\n",
    "    \"generated control\"\n",
    "    \"\"\"\n",
    "    if control_params is None:\n",
    "        control_params = {\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'num_beams': 4,\n",
    "            'repetition_penalty': 1.2,\n",
    "            'length_penalty': 1.0,\n",
    "            'max_length': 150,\n",
    "            'min_length': 30,\n",
    "        }\n",
    "    \n",
    "    # Buat prompt untuk inference - lebih singkat\n",
    "    prompt = f\"Summarize this article: {text[:400]}\\nSummary:\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Pindah ke device yang sama dengan model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate dengan kontrol parameter\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=control_params['max_length'],\n",
    "            min_length=control_params['min_length'],\n",
    "            \n",
    "            # üî• GENERATED CONTROL PARAMETERS\n",
    "            temperature=control_params['temperature'],\n",
    "            top_p=control_params['top_p'],\n",
    "            num_beams=control_params['num_beams'],\n",
    "            repetition_penalty=control_params['repetition_penalty'],\n",
    "            length_penalty=control_params['length_penalty'],\n",
    "            \n",
    "            # Other parameters\n",
    "            do_sample=True,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=3,  # Hindari pengulangan n-gram\n",
    "        )\n",
    "    \n",
    "    # Decode dan bersihkan output\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Hapus prompt dari output\n",
    "    if prompt in summary:\n",
    "        summary = summary.replace(prompt, \"\").strip()\n",
    "    \n",
    "    # Bersihkan teks\n",
    "    summary = summary.split('\\n')[0]  # Ambil baris pertama saja\n",
    "    summary = summary.strip()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Test dengan beberapa contoh dari test set\n",
    "print(\"\\nüß™ Testing model with controlled generation...\")\n",
    "\n",
    "# Ambil sampel untuk testing (lebih sedikit untuk demo)\n",
    "test_samples = []\n",
    "for i in range(10):\n",
    "    original_sample = dataset['test'][i]\n",
    "    test_samples.append({\n",
    "        'document': original_sample['document'],\n",
    "        'summary': original_sample['summary'],\n",
    "        'id': original_sample['id']\n",
    "    })\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(\"\\nüìù Generated Summaries (with control parameters):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, sample in enumerate(test_samples[:5]):  # Hanya tampilkan 5 contoh\n",
    "    text = sample['document']\n",
    "    ref = sample['summary']\n",
    "    \n",
    "    # Generate dengan kontrol parameter\n",
    "    try:\n",
    "        pred = generate_summary_with_control(text, model, tokenizer)\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "        \n",
    "        print(f\"\\nüìå Contoh {i+1}:\")\n",
    "        print(f\"üìÑ Dokumen (potongan): {text[:150]}...\")\n",
    "        print(f\"üéØ Referensi (ground truth): {ref}\")\n",
    "        print(f\"ü§ñ Prediksi (model): {pred}\")\n",
    "        print(\"-\" * 80)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating summary for sample {i}: {e}\")\n",
    "        # Tambahkan placeholder jika error\n",
    "        predictions.append(\"Error in generation\")\n",
    "        references.append(ref)\n",
    "\n",
    "# Calculate ROUGE scores jika ada predictions\n",
    "if len(predictions) > 0 and any(pred != \"Error in generation\" for pred in predictions):\n",
    "    print(\"\\nüìä Calculating ROUGE scores...\")\n",
    "    \n",
    "    # Filter out error predictions\n",
    "    valid_indices = [i for i, pred in enumerate(predictions) if pred != \"Error in generation\"]\n",
    "    if valid_indices:\n",
    "        valid_predictions = [predictions[i] for i in valid_indices]\n",
    "        valid_references = [references[i] for i in valid_indices]\n",
    "        \n",
    "        try:\n",
    "            results = rouge.compute(\n",
    "                predictions=valid_predictions,\n",
    "                references=valid_references,\n",
    "                use_stemmer=True,\n",
    "                use_aggregator=True\n",
    "            )\n",
    "            \n",
    "            print(\"\\n‚úÖ EVALUATION RESULTS:\")\n",
    "            print(f\"   ROUGE-1: {results['rouge1']:.4f}\")\n",
    "            print(f\"   ROUGE-2: {results['rouge2']:.4f}\")\n",
    "            print(f\"   ROUGE-L: {results['rougeL']:.4f}\")\n",
    "            if 'rougeLsum' in results:\n",
    "                print(f\"   ROUGE-Lsum: {results['rougeLsum']:.4f}\")\n",
    "            \n",
    "            # Save results\n",
    "            print(\"\\nüíæ Saving evaluation results...\")\n",
    "            results_df = pd.DataFrame({\n",
    "                'document_id': [test_samples[i]['id'] for i in valid_indices],\n",
    "                'document_preview': [test_samples[i]['document'][:200] + \"...\" for i in valid_indices],\n",
    "                'reference_summary': valid_references,\n",
    "                'generated_summary': valid_predictions,\n",
    "                'rouge1': results['rouge1'],\n",
    "                'rouge2': results['rouge2'],\n",
    "                'rougeL': results['rougeL'],\n",
    "            })\n",
    "            # Buat folder results jika belum ada\n",
    "            import os\n",
    "            os.makedirs(\"./results\", exist_ok=True)\n",
    "            results_df.to_csv(\"./results/xsum_summarization_results.csv\", index=False)\n",
    "            print(f\"‚úÖ Results saved to: ./results/xsum_summarization_results.csv\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calculating ROUGE: {e}\")\n",
    "            print(\"‚ö†Ô∏è  Showing raw predictions instead:\")\n",
    "            for i, (pred, ref) in enumerate(zip(valid_predictions, valid_references)):\n",
    "                print(f\"\\nSample {i+1}:\")\n",
    "                print(f\"Reference: {ref}\")\n",
    "                print(f\"Prediction: {pred}\")\n",
    "else:\n",
    "    print(\"‚ùå No valid predictions generated for evaluation\")\n",
    "\n",
    "# DEMONSTRASI GENERATED CONTROL YANG LEBIH SEDERHANA\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéõÔ∏è  DEMONSTRATION: GENERATED CONTROL PARAMETERS (Simple)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Gunakan sampel yang lebih pendek untuk demo\n",
    "demo_sample = test_samples[0]\n",
    "demo_text = demo_sample['document'][:300]  # Hanya 300 karakter untuk demo\n",
    "\n",
    "print(f\"\\nüìÑ Demo document (truncated): {demo_text}...\")\n",
    "\n",
    "# Demo dengan parameter yang berbeda\n",
    "print(\"\\nüîß Effect of temperature control:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for temp in [0.3, 0.7, 1.0]:\n",
    "    control_params = {\n",
    "        'temperature': temp,\n",
    "        'top_p': 0.9,\n",
    "        'num_beams': 2,\n",
    "        'repetition_penalty': 1.2,\n",
    "        'length_penalty': 1.0,\n",
    "        'max_length': 100,\n",
    "        'min_length': 20,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        summary = generate_summary_with_control(\n",
    "            demo_text, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            control_params\n",
    "        )\n",
    "        print(f\"\\nüå°Ô∏è  Temperature = {temp}:\")\n",
    "        print(f\"   {summary}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error with temp={temp}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ EVALUATION COMPLETED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91846577",
   "metadata": {},
   "source": [
    "## 8. DEMONSTRASI GENERATED CONTROL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0ea3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéõÔ∏è  DEMONSTRATION: GENERATED CONTROL PARAMETERS\n",
      "======================================================================\n",
      "\n",
      "üìÑ Demo document: The move is in response to an ¬£8m cut in the subsidy received from the Department of Employment and Learning (DEL).\n",
      "The cut in undergraduate places will come into effect from September 2015.\n",
      "Job losse...\n",
      "\n",
      "üîß Effect of different control parameters:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ö° Conservative Settings:\n",
      "   ‚Ä¢ Temperature: 0.3\n",
      "   ‚Ä¢ Top-p: 0.5\n",
      "   ‚Ä¢ Beams: 1\n",
      "   ‚Ä¢ Max length: 100\n",
      "   Summary: DEL has been a key part for many\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ö° Balanced Settings:\n",
      "   ‚Ä¢ Temperature: 0.7\n",
      "   ‚Ä¢ Top-p: 0.9\n",
      "   ‚Ä¢ Beams: 4\n",
      "   ‚Ä¢ Max length: 150\n",
      "   Summary: The cuts are part of an increase in the ¬£8.5bn DEL budget for the UK economy.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ö° Creative Settings:\n",
      "   ‚Ä¢ Temperature: 1.0\n",
      "   ‚Ä¢ Top-p: 0.95\n",
      "   ‚Ä¢ Beams: 1\n",
      "   ‚Ä¢ Max length: 200\n",
      "   Summary: For all UK universities who receive public funding we would still see it as a pay increase for two years but with little justification or financial savings possible because they could make more cuts on their costs over time...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üéØ Demonstration: Effect of Repetition Penalty\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîÅ Repetition Penalty = 1.0:\n",
      "   The cuts will be in place for the next three years.\n",
      "\n",
      "üîÅ Repetition Penalty = 1.2:\n",
      "   The cuts will be made in the first quarter of 2016.\n",
      "\n",
      "üîÅ Repetition Penalty = 1.5:\n",
      "   The DEL cuts will be in place for the first time since 2014.\n",
      "\n",
      "üîÅ Repetition Penalty = 2.0:\n",
      "   The cuts were announced by the DEL on Wednesday.\n",
      "\n",
      "======================================================================\n",
      "üìã SUMMARY OF GENERATED CONTROL PARAMETERS:\n",
      "======================================================================\n",
      "\n",
      "1. **Temperature** (0.3-1.0):\n",
      "   - Rendah (0.3): Output deterministik, konservatif\n",
      "   - Tinggi (1.0): Output kreatif, beragam\n",
      "\n",
      "2. **Top-p (Nucleus Sampling)** (0.5-0.95):\n",
      "   - Mengontrol variasi token yang dipertimbangkan\n",
      "   - 0.9: Pertimbangkan 90% token teratas\n",
      "\n",
      "3. **Beam Search** (1-4):\n",
      "   - 1: Greedy decoding (cepat)\n",
      "   - 4: Beam search (lebih baik, lebih lambat)\n",
      "\n",
      "4. **Repetition Penalty** (1.0-2.0):\n",
      "   - 1.0: Tidak ada penalti\n",
      "   - >1.0: Kurangi pengulangan token\n",
      "\n",
      "5. **Length Parameters**:\n",
      "   - min_length: Panjang minimum summary\n",
      "   - max_length: Panjang maximum summary\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úÖ GENERATED CONTROL DEMONSTRATION COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéõÔ∏è  DEMONSTRATION: GENERATED CONTROL PARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def generate_summary_with_control_fixed(text, model, tokenizer, control_params=None):\n",
    "    \"\"\"\n",
    "    Generate summary dengan kontrol parameter - FIXED VERSION\n",
    "    \"\"\"\n",
    "    # Default parameters jika tidak disediakan\n",
    "    default_params = {\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.9,\n",
    "        'num_beams': 4,\n",
    "        'repetition_penalty': 1.2,\n",
    "        'length_penalty': 1.0,\n",
    "        'max_length': 150,\n",
    "        'min_length': 30,\n",
    "    }\n",
    "    \n",
    "    # Merge default dengan user params\n",
    "    if control_params is None:\n",
    "        control_params = default_params\n",
    "    else:\n",
    "        for key, value in default_params.items():\n",
    "            if key not in control_params:\n",
    "                control_params[key] = value\n",
    "    \n",
    "    # Buat prompt untuk inference\n",
    "    prompt = f\"Summarize this article: {text[:400]}\\nSummary:\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Pindah ke device yang sama dengan model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate dengan kontrol parameter\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=control_params['max_length'],\n",
    "            min_length=control_params['min_length'],\n",
    "            temperature=control_params['temperature'],\n",
    "            top_p=control_params['top_p'],\n",
    "            num_beams=control_params['num_beams'],\n",
    "            repetition_penalty=control_params['repetition_penalty'],\n",
    "            length_penalty=control_params['length_penalty'],\n",
    "            do_sample=control_params['temperature'] > 0,  # Sample jika temperature > 0\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "    \n",
    "    # Decode dan bersihkan output\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Hapus prompt dari output\n",
    "    if prompt in summary:\n",
    "        summary = summary.replace(prompt, \"\").strip()\n",
    "    \n",
    "    # Bersihkan teks\n",
    "    summary = summary.split('\\n')[0]  # Ambil baris pertama saja\n",
    "    summary = summary.strip()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Demo efek parameter kontrol yang berbeda\n",
    "demo_text = dataset['test'][10]['document'][:500] + \"...\"\n",
    "\n",
    "print(f\"\\nüìÑ Demo document: {demo_text[:200]}...\")\n",
    "\n",
    "# Konfigurasi yang lengkap dengan semua parameter\n",
    "control_configs = [\n",
    "    {\n",
    "        'name': 'Conservative', \n",
    "        'temp': 0.3, \n",
    "        'top_p': 0.5, \n",
    "        'beams': 1,\n",
    "        'max_length': 100,\n",
    "        'min_length': 20,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Balanced', \n",
    "        'temp': 0.7, \n",
    "        'top_p': 0.9, \n",
    "        'beams': 4,\n",
    "        'max_length': 150,\n",
    "        'min_length': 30,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Creative', \n",
    "        'temp': 1.0, \n",
    "        'top_p': 0.95, \n",
    "        'beams': 1,\n",
    "        'max_length': 200,\n",
    "        'min_length': 40,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nüîß Effect of different control parameters:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for config in control_configs:\n",
    "    # Buat control_params dengan SEMUA parameter yang diperlukan\n",
    "    control_params = {\n",
    "        'temperature': config['temp'],\n",
    "        'top_p': config['top_p'],\n",
    "        'num_beams': config['beams'],\n",
    "        'repetition_penalty': 1.2,\n",
    "        'length_penalty': 1.0,\n",
    "        'max_length': config['max_length'],  # ‚úÖ Ditambahkan\n",
    "        'min_length': config['min_length'],  # ‚úÖ Ditambahkan\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        summary = generate_summary_with_control_fixed(\n",
    "            demo_text, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            control_params\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚ö° {config['name']} Settings:\")\n",
    "        print(f\"   ‚Ä¢ Temperature: {config['temp']}\")\n",
    "        print(f\"   ‚Ä¢ Top-p: {config['top_p']}\")\n",
    "        print(f\"   ‚Ä¢ Beams: {config['beams']}\")\n",
    "        print(f\"   ‚Ä¢ Max length: {config['max_length']}\")\n",
    "        print(f\"   Summary: {summary}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error with {config['name']} settings: {e}\")\n",
    "        print(f\"   Parameters used: {control_params}\")\n",
    "\n",
    "# Demo tambahan: efek repetition penalty\n",
    "print(\"\\nüéØ Demonstration: Effect of Repetition Penalty\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rep_penalty in [1.0, 1.2, 1.5, 2.0]:\n",
    "    control_params = {\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.9,\n",
    "        'num_beams': 2,\n",
    "        'repetition_penalty': rep_penalty,\n",
    "        'length_penalty': 1.0,\n",
    "        'max_length': 120,\n",
    "        'min_length': 25,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        summary = generate_summary_with_control_fixed(\n",
    "            demo_text, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            control_params\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüîÅ Repetition Penalty = {rep_penalty}:\")\n",
    "        print(f\"   {summary}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error with rep_penalty={rep_penalty}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã SUMMARY OF GENERATED CONTROL PARAMETERS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. **Temperature** (0.3-1.0):\n",
    "   - Rendah (0.3): Output deterministik, konservatif\n",
    "   - Tinggi (1.0): Output kreatif, beragam\n",
    "\n",
    "2. **Top-p (Nucleus Sampling)** (0.5-0.95):\n",
    "   - Mengontrol variasi token yang dipertimbangkan\n",
    "   - 0.9: Pertimbangkan 90% token teratas\n",
    "\n",
    "3. **Beam Search** (1-4):\n",
    "   - 1: Greedy decoding (cepat)\n",
    "   - 4: Beam search (lebih baik, lebih lambat)\n",
    "\n",
    "4. **Repetition Penalty** (1.0-2.0):\n",
    "   - 1.0: Tidak ada penalti\n",
    "   - >1.0: Kurangi pengulangan token\n",
    "\n",
    "5. **Length Parameters**:\n",
    "   - min_length: Panjang minimum summary\n",
    "   - max_length: Panjang maximum summary\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ GENERATED CONTROL DEMONSTRATION COMPLETED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496a3bd",
   "metadata": {},
   "source": [
    "## 9. GENERATE FINAL REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "695bdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update bagian Evaluation Results di report\n",
    "report_content = f\"\"\"\n",
    "# TASK 3: DECODER-ONLY LLM FOR ABSTRACTIVE SUMMARIZATION\n",
    "\n",
    "## Project Overview\n",
    "Fine-tuned a decoder-only LLM (DistilGPT-2 as efficient alternative to PIN-2) on the XSum dataset\n",
    "for abstractive summarization task.\n",
    "\n",
    "## Hardware Constraints & Adaptations\n",
    "- **GPU**: NVIDIA GeForce RTX 3050 Ti Laptop GPU (4GB VRAM)\n",
    "- **Adaptations Made**:\n",
    "  1. Used DistilGPT-2 instead of full GPT-2 (40% smaller)\n",
    "  2. Reduced batch size to 1 with gradient accumulation\n",
    "  3. Enabled gradient checkpointing and mixed precision\n",
    "  4. Used subset of data (5,000 samples) for feasible training\n",
    "\n",
    "## Model Details\n",
    "- **Base Model**: {MODEL_NAME} (DistilGPT-2)\n",
    "- **Parameters**: {sum(p.numel() for p in model.parameters()):,}\n",
    "- **Fine-tuning Approach**: Causal language modeling with instruction-style prompting\n",
    "- **Dataset**: XSum (BBC news articles with one-sentence summaries)\n",
    "- **Training Samples**: {len(train_dataset)} (subset for feasibility)\n",
    "- **Validation Samples**: {len(val_dataset)}\n",
    "\n",
    "## Training Configuration\n",
    "- **Epochs**: {training_args.num_train_epochs}\n",
    "- **Batch Size**: {training_args.per_device_train_batch_size}\n",
    "- **Gradient Accumulation**: {training_args.gradient_accumulation_steps}\n",
    "- **Learning Rate**: {training_args.learning_rate}\n",
    "- **Optimizer**: AdamW with weight decay\n",
    "\n",
    "## Generated Control Parameters (Successfully Implemented)\n",
    "1. **Temperature**: 0.3-1.0 (controls randomness)\n",
    "2. **Top-p**: 0.5-0.95 (nucleus sampling)\n",
    "3. **Beam Search**: 1-4 beams\n",
    "4. **Repetition Penalty**: 1.0-2.0\n",
    "5. **Length Control**: min_length, max_length\n",
    "\n",
    "## Evaluation Results\n",
    "- **ROUGE-1**: {results['rouge1']:.4f}\n",
    "- **ROUGE-2**: {results['rouge2']:.4f} \n",
    "- **ROUGE-L**: {results['rougeL']:.4f}\n",
    "- **ROUGE-Lsum**: {results['rougeLsum']:.4f}\n",
    "\n",
    "## Analysis of Results\n",
    "The ROUGE scores are lower than expected due to:\n",
    "1. **Hardware limitations**: 4GB GPU restricted model size and batch size\n",
    "2. **Training time**: Limited to 3 epochs for feasibility\n",
    "3. **Data subset**: Used 5,000 samples instead of full 204,045\n",
    "\n",
    "## Key Features Implemented (All Requirements Met)\n",
    "‚úÖ Decoder-only LLM architecture (DistilGPT-2 as PIN-2 equivalent)\n",
    "‚úÖ Instruction-style prompting for summarization  \n",
    "‚úÖ Causal language modeling training approach\n",
    "‚úÖ Generated control parameters for inference\n",
    "‚úÖ Abstractive summarization task\n",
    "‚úÖ XSum dataset implementation\n",
    "‚úÖ ROUGE evaluation metrics\n",
    "\n",
    "## Lessons Learned & Future Improvements\n",
    "1. **With more GPU memory**: Use full GPT-2 and larger batch size\n",
    "2. **With more time**: Train on full dataset for 5+ epochs\n",
    "3. **Architecture**: Try encoder-decoder models (T5, BART) for better summarization\n",
    "4. **Prompt engineering**: Experiment with different prompt formats\n",
    "\n",
    "## Repository Structure\n",
    "- `models/`: Fine-tuned model and tokenizer\n",
    "- `results/`: Evaluation results and predictions\n",
    "- `notebooks/`: This Jupyter notebook\n",
    "- `src/`: Source code for preprocessing and training\n",
    "- `requirements.txt`: Python dependencies\n",
    "\n",
    "## How to Reproduce\n",
    "1. Install: `pip install -r requirements.txt`\n",
    "2. Run notebook cells sequentially\n",
    "3. Training time: ~3 hours on RTX 3050 Ti 4GB\n",
    "4. Evaluation: Uses 500 test samples\n",
    "\n",
    "---\n",
    "*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "*Note: Results reflect hardware constraints - academic exercise successful*\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50af4d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã GENERATING FINAL REPORT...\n",
      "‚úÖ Final report saved to: ./results/task3_final_report.md\n",
      "\n",
      "======================================================================\n",
      "üìÅ FILES GENERATED FOR SUBMISSION:\n",
      "======================================================================\n",
      "1. Fine-tuned model: ./models/gpt2-xsum-finetuned/\n",
      "2. Evaluation results: ./results/xsum_summarization_results.csv\n",
      "3. Final report: ./results/task3_final_report.md\n",
      "4. Training logs: ./logs/\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìã GENERATING FINAL REPORT...\")\n",
    "\n",
    "report_content = f\"\"\"\n",
    "# TASK 3: DECODER-ONLY LLM FOR ABSTRACTIVE SUMMARIZATION\n",
    "\n",
    "## Project Overview\n",
    "Fine-tuned a decoder-only LLM (GPT-2 as PIN-2 equivalent) on the XSum dataset\n",
    "for abstractive summarization task.\n",
    "\n",
    "## Model Details\n",
    "- **Base Model**: {MODEL_NAME}\n",
    "- **Fine-tuning Approach**: Causal language modeling with instruction-style prompting\n",
    "- **Dataset**: XSum (BBC news articles with one-sentence summaries)\n",
    "- **Training Samples**: {len(train_dataset)}\n",
    "- **Validation Samples**: {len(val_dataset)}\n",
    "\n",
    "## Training Configuration\n",
    "- **Epochs**: {training_args.num_train_epochs}\n",
    "- **Batch Size**: {training_args.per_device_train_batch_size}\n",
    "- **Learning Rate**: {training_args.learning_rate}\n",
    "- **Optimizer**: AdamW with weight decay\n",
    "\n",
    "## Generated Control Parameters\n",
    "The model supports various generation control parameters:\n",
    "1. **Temperature**: Controls randomness (0.3-1.0)\n",
    "2. **Top-p**: Nucleus sampling parameter (0.5-0.95)\n",
    "3. **Beam Search**: Multiple beams for better quality\n",
    "4. **Repetition Penalty**: Prevents repetitive text\n",
    "5. **Length Penalty**: Controls summary length\n",
    "\n",
    "## Evaluation Results\n",
    "- **ROUGE-1**: {results['rouge1']:.4f}\n",
    "- **ROUGE-2**: {results['rouge2']:.4f}\n",
    "- **ROUGE-L**: {results['rougeL']:.4f}\n",
    "- **ROUGE-Lsum**: {results['rougeLsum']:.4f}\n",
    "\n",
    "## Key Features Implemented\n",
    "‚úÖ Decoder-only LLM architecture (GPT-2)\n",
    "‚úÖ Instruction-style prompting for summarization\n",
    "‚úÖ Causal language modeling training approach\n",
    "‚úÖ Generated control parameters for inference\n",
    "‚úÖ Abstractive summarization (not extractive)\n",
    "‚úÖ XSum dataset compatibility\n",
    "\n",
    "## Repository Structure\n",
    "- `models/`: Fine-tuned model and tokenizer\n",
    "- `results/`: Evaluation results and predictions\n",
    "- `notebooks/`: Training and evaluation notebooks\n",
    "- `src/`: Source code for preprocessing and training\n",
    "- `requirements.txt`: Python dependencies\n",
    "\n",
    "## How to Use\n",
    "1. Load the fine-tuned model: `AutoModelForCausalLM.from_pretrained('./models/gpt2-xsum-finetuned')`\n",
    "2. Use `generate_summary_with_control()` function for inference\n",
    "3. Adjust control parameters for different summarization styles\n",
    "\n",
    "---\n",
    "*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "with open(\"./results/task3_final_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(\"‚úÖ Final report saved to: ./results/task3_final_report.md\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìÅ FILES GENERATED FOR SUBMISSION:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Fine-tuned model: ./models/gpt2-xsum-finetuned/\")\n",
    "print(\"2. Evaluation results: ./results/xsum_summarization_results.csv\")\n",
    "print(\"3. Final report: ./results/task3_final_report.md\")\n",
    "print(\"4. Training logs: ./logs/\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert_env_new)",
   "language": "python",
   "name": "bert_env_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
